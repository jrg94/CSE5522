{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lab2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNURULxkoHI82qv5mIKS/hG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jrg94/CSE5522/blob/lab2/lab2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3Yg-tYejPDe",
        "colab_type": "text"
      },
      "source": [
        "# CSE 5522 - Lab 2\n",
        "By Jeremy Grifski"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5c9Rh8njZyn",
        "colab_type": "text"
      },
      "source": [
        "In this lab, we'll be taking a look at sentiment analysis of tweet data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OqVwOqZwjn79",
        "colab_type": "text"
      },
      "source": [
        "## Part 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQ4zl1LukMCk",
        "colab_type": "text"
      },
      "source": [
        "**1.0** Set up the environment (you can click on the play button below to import the appropriate modules)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q7jdntWtjOJn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jagDaH_IkH3m",
        "colab_type": "text"
      },
      "source": [
        "**1.1** Read the data from GitHub into a pandas dataframe.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wg6ioz-BkW4e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TweetUrl='https://github.com/aasiaeet/cse5522data/raw/master/db3_final_clean.csv'\n",
        "tweet_dataframe=pd.read_csv(TweetUrl)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ukzdqt07lx6f",
        "colab_type": "text"
      },
      "source": [
        "**1.2** Print out the top of the dataframe to make sure that the data loaded correctly. It should be a data table with three columns (weight, tweet, label), and 3697 rows."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LEWVXZaYl0k1",
        "colab_type": "code",
        "outputId": "7bad151f-2162-4d1f-c238-f303fb5bbc15",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        }
      },
      "source": [
        "display(tweet_dataframe.shape)\n",
        "tweet_dataframe.head()"
      ],
      "execution_count": 209,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(3697, 3)"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>weight</th>\n",
              "      <th>tweet</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.0000</td>\n",
              "      <td>it is very cold out want it to be warmer</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.7698</td>\n",
              "      <td>dammmmmmm its pretty cold this morning   burr lol</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.6146</td>\n",
              "      <td>why does halsey have to be so far away think m...</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.9356</td>\n",
              "      <td>dammit stop being so cold so can work out</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.0000</td>\n",
              "      <td>its too freakin cold</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   weight                                              tweet  label\n",
              "0  1.0000          it is very cold out want it to be warmer      -1\n",
              "1  0.7698  dammmmmmm its pretty cold this morning   burr lol     -1\n",
              "2  0.6146  why does halsey have to be so far away think m...     -1\n",
              "3  0.9356         dammit stop being so cold so can work out      -1\n",
              "4  1.0000                               its too freakin cold     -1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 209
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNd7KaYnqBFT",
        "colab_type": "text"
      },
      "source": [
        "Labels are -1 and +1 for negative and positive sentiments respectively. Multiple judges have been asked to choose a label for a tweet (this is an example of crowd-sourcing) from five possible labels:\n",
        "\n",
        "- Tweet is not relevant to weather.\n",
        "- I can't tell the sentiment.\n",
        "- Neutral: author just sharing information.\n",
        "- Positive\n",
        "- Negative\n",
        "\n",
        "\n",
        "The majority vote was picked as the label and its ratio was set as the weight of the tweet. So for the tweet in row 2 above, 61% of judges voted that the label is negative.\n",
        "\n",
        "Note that tweets have been pre-processed (or cleaned). For example, :) and :( :) were replaced with \"sad\" and \"smiley\" and numbers with \"num\", etc. You can go further (as we ask in 1.12) and remove the stop words, i.e., repetitive non-informative words such as am, is, and are."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dm5TRWzaqG-b",
        "colab_type": "text"
      },
      "source": [
        "**1.3** In the next step, we should build our feature matrix by converting the string of words to a vector of numeric values.\n",
        "\n",
        "First we need to assign a unique id to each word and create the feature matrix with correct size:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVVzeHL-qMOb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# wordDict maps words to id\n",
        "# X is the document-word matrix holding the presence/absence of words in each tweet\n",
        "wordDict = {}\n",
        "idCounter = 0\n",
        "for i in range(tweet_dataframe.shape[0]):\n",
        "  allWords = tweet_dataframe.iloc[i,1].split(\" \")\n",
        "  for word in allWords:\n",
        "    if word not in wordDict:\n",
        "      wordDict[word] = idCounter\n",
        "      idCounter += 1\n",
        "X = np.zeros((tweet_dataframe.shape[0], idCounter),dtype='float')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_e8F8XJpqzrN",
        "colab_type": "text"
      },
      "source": [
        "Checking head of the dictionary:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwPwu3lDq1WL",
        "colab_type": "code",
        "outputId": "1cd651a4-13d4-4ec0-9f6f-da4cf6d9d290",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "source": [
        "dict(list(wordDict.items())[0:10])"
      ],
      "execution_count": 211,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'': 9,\n",
              " 'be': 7,\n",
              " 'cold': 3,\n",
              " 'is': 1,\n",
              " 'it': 0,\n",
              " 'out': 4,\n",
              " 'to': 6,\n",
              " 'very': 2,\n",
              " 'want': 5,\n",
              " 'warmer': 8}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 211
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3XUpzZ_q3WM",
        "colab_type": "text"
      },
      "source": [
        "**1.4** The simplest way of coding a tweet to numbers is to mark the occurrence of a word, and forget about its frequency in the document (tweet). This works well with tweets as there are not many repetitive words in a single tweet. So let's fill the document-word matrix:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yuGZb8uUq6Mr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(tweet_dataframe.shape[0]):\n",
        "  allWords = tweet_dataframe.iloc[i,1].split(\" \")\n",
        "  for word in allWords:\n",
        "    X[i, wordDict[word]]  = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FmzAkh2ErEkk",
        "colab_type": "text"
      },
      "source": [
        "Now we check if the number of words are correct:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d08d8GrHrGcr",
        "colab_type": "code",
        "outputId": "ecdff789-38e5-4dcf-950d-36cdc2d6ce1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "np.sum(X[0:5, ], axis = 1)"
      ],
      "execution_count": 213,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([10.,  9., 17.,  9.,  4.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 213
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wWc1zWxrH-1",
        "colab_type": "text"
      },
      "source": [
        "Finally, we extract the labels from the dataframe:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNBuFCJzrJ8E",
        "colab_type": "code",
        "outputId": "98f8ecbc-06ab-4cdb-c76b-af7061193680",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y = np.array(tweet_dataframe.iloc[:,2])\n",
        "y[0:5]"
      ],
      "execution_count": 214,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-1, -1, -1, -1, -1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 214
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xLNy-zArMAt",
        "colab_type": "text"
      },
      "source": [
        "Let's compute the total number of positive and negative tweets:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tawyq0NJrOLc",
        "colab_type": "code",
        "outputId": "59d42368-d234-48cd-89ea-b84fcad7a527",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "numNeg = np.sum(y<0)\n",
        "numPos = np.sum(y>=0) #len(y) - numNeg\n",
        "probNeg = numNeg / (numNeg + numPos)\n",
        "probPos = 1 - probNeg\n",
        "display(numNeg, numPos, probNeg, probPos)"
      ],
      "execution_count": 215,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "1650"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "2047"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0.4463078171490398"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0.5536921828509602"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WY-ukqndrP9s",
        "colab_type": "text"
      },
      "source": [
        "So samples 0:1649 are negative and 1650:-1 are positive."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQHG-cg8Rm3I",
        "colab_type": "text"
      },
      "source": [
        "**1.5** Train/Test Split Now with do the 20/80 split and learn the word probabilities using the 80 % part and test the NB performance on the 20 % part."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sb-bUT-URsRd",
        "colab_type": "code",
        "outputId": "dc10dc47-af9e-4911-dd36-8e2ab4788ca2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "xTrain, xTest, yTrain, yTest = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
        "display(xTrain.shape, xTest.shape, yTrain.shape, yTest.shape)\n",
        "#Note: random_state=0 fixes the random seed so we get the same split every run. Don't use this below"
      ],
      "execution_count": 216,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(2957, 5989)"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(740, 5989)"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(2957,)"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(740,)"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nt2heJYDRvzn",
        "colab_type": "text"
      },
      "source": [
        "**1.6** Computing Probabilities by Counting Now the real work begins. Write the code that, from the train feature matrix xTrain computes the needed word probabilites, i.e.,  P(word|label)  where label is + or - and word is any of the words saved in the wordDict:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWwubhL9R1I_",
        "colab_type": "code",
        "outputId": "5ce4ba3a-3d3f-488f-922f-9407003c0e7f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "# compute three distributions (four variables):\n",
        "def compute_distros(x,y):\n",
        "  # probWordGivenPositive: P(word|Sentiment = +ive)\n",
        "  probWordGivenPositive = np.sum(x[y >= 0,:], axis=0) #Sum each word (column) to count how many times each word shows up (in positive examples)\n",
        "  probWordGivenPositive = probWordGivenPositive / np.sum(y >= 0) #Divide by total number of (positive) examples to give distribution\n",
        "\n",
        "  # probWordGivenNegative: P(word|Sentiment = -ive)\n",
        "  probWordGivenNegative = np.sum(x[y < 0,:], axis=0)\n",
        "  probWordGivenNegative = probWordGivenNegative / np.sum(y < 0)\n",
        "\n",
        "  # priorPositive: P(Sentiment = +ive)\n",
        "  priorPositive = np.sum(y >= 0) / y.shape[0] #Number of positive examples vs. all examples\n",
        "  # priorNegative: P(Sentiment = -ive)\n",
        "  priorNegative = 1 - priorPositive\n",
        "  #  (note these last two form one distribution)\n",
        "\n",
        "  return probWordGivenPositive, probWordGivenNegative, priorPositive, priorNegative\n",
        "\n",
        "# compute distributions here\n",
        "probWordGivenPositive, probWordGivenNegative, priorPositive, priorNegative = compute_distros(xTrain,yTrain)\n",
        "\n",
        "# checking the results\n",
        "display(probWordGivenPositive[0:5])\n",
        "display(probWordGivenNegative[0:5])\n",
        "display(priorPositive, priorNegative)"
      ],
      "execution_count": 217,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([0.1185006 , 0.20737606, 0.01088271, 0.01451028, 0.10217654])"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([0.14504988, 0.19493477, 0.00537222, 0.09669992, 0.13967767])"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0.5593506932702063"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0.44064930672979374"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JUFU8eUQ_8gC",
        "colab_type": "text"
      },
      "source": [
        "Note that you only needed to compute $P(word = 1| +)$ or $P(word = 1| -)$ and the probabilities of the word being absent from a tweet is just 1 minus those probabilities. \n",
        "\n",
        "However, as we see in 1.7, for convenience, we will also want to compute $log P(word = 1 | +)$, $log P(word = 0 | +)$, $log P(word = 1 | -)$ and $log P(word = 0 | -)$.  Also we should compute the log priors.  Let's do so now.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1HLcaaDTiwF0",
        "colab_type": "code",
        "outputId": "df3c968b-2c64-4678-cf5b-63037d7be8f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "# compute the following:\n",
        "# logProbWordPresentGivenPositive\n",
        "# logProbWordAbsentGivenPositive\n",
        "# logProbWordPresentGivenNegative\n",
        "# logProbWordAbsentGivenNegative\n",
        "# logPriorPositive\n",
        "# logPriorNegative\n",
        "def compute_logdistros(distros, min_prob):\n",
        "  if True:\n",
        "    #Assume missing words are simply very rare\n",
        "    #So, assign minimum probability to very small elements (e.g. 0 elements)\n",
        "    distros = np.where(distros >= min_prob, distros, min_prob)\n",
        "    #Also need to consider minimum probability for \"not\" distribution\n",
        "    distros = np.where(distros <= (1 - min_prob), distros, 1 - min_prob)\n",
        "\n",
        "    return np.log(distros), np.log(1 - distros)\n",
        "  else:\n",
        "    #Ignore missing words (assume they have P==1, i.e. force log 0 to 0)\n",
        "    return np.log(np.where(distros>0,distros,1)), np.log(np.where(distros<1,1-distros,1))\n",
        "\n",
        "min_prob = 1 / yTrain.shape[0] # Assume very rare words only appeared once\n",
        "logProbWordPresentGivenPositive, logProbWordAbsentGivenPositive = compute_logdistros(probWordGivenPositive, min_prob)\n",
        "logProbWordPresentGivenNegative, logProbWordAbsentGivenNegative = compute_logdistros(probWordGivenNegative, min_prob)\n",
        "logPriorPositive, logPriorNegative = compute_logdistros(priorPositive,min_prob)\n",
        "\n",
        "# Did this work, or did you get an error?  (Read below.)\n",
        "display(logProbWordPresentGivenPositive[0:5])\n",
        "display(logProbWordAbsentGivenPositive[0:5])\n",
        "display(logProbWordPresentGivenNegative[0:5])\n",
        "display(logProbWordAbsentGivenNegative[0:5])\n",
        "display(logPriorPositive, logPriorNegative)"
      ],
      "execution_count": 218,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([-2.13283722, -1.57322143, -4.52058012, -4.23289805, -2.28105316])"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([-0.12613096, -0.23240639, -0.01094236, -0.01461658, -0.10778182])"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([-1.93067756, -1.63509031, -5.22651443, -2.33614267, -1.96841789])"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([-0.15671216, -0.21683197, -0.0053867 , -0.10170047, -0.15044815])"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "-0.5809786442688406"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "-0.819505942727632"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXVQ7ZHAkH1u",
        "colab_type": "text"
      },
      "source": [
        "You likely received an error when you tried to take $log(0)$ at some point.  Can your group think of a way to avoid taking $log(0)$?  Check in with your instructor/TA to see if what you're thinking will work.  Implement that change in your code above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxdbYsu9E8av",
        "colab_type": "text"
      },
      "source": [
        "**1.7: Math of NB** Here we provide the derivation of NB when we want to classify the $i$th tweet $\\textbf{x}^{(i)}$ and the size of dictionary is $p$, i.e., each tweet is a binary vector of size $p$ as $\\textbf{x}^{(i)} = (x_1^{(i)},\\dots, x_p^{(i)})$. \n",
        "\n",
        "Note that we computed $P(x_j^{(i)} = 1|+)$ and $P(x_j^{(i)} = 1|-)$ in above code from `xTrain` and now want to classify `xTest` samples.\n",
        "\n",
        "**Classification Rule:** For each tweet $i$ NB classifier assigns label + if $P(+|\\textbf{x}^{(i)}) > P(-|\\textbf{x}^{(i)})$ and negative otherwise. \n",
        "\n",
        "These posterior probabilities can be computed using prior probabilities (that we got from `xTrain`) and Bayes rule as follows: \n",
        "\n",
        "\\begin{align}\n",
        "P(+|\\textbf{x}^{(i)}) &= \\alpha P(\\{\\textbf{x}^{(i)}\\}_{i=1}^n | +)P(+) \n",
        "\\\\\n",
        "(\\text{NB Assumption}) &= \\alpha P(+) \\prod_{j=1}^p P(x_j^{(i)}|+)\n",
        "\\end{align}\n",
        "\n",
        "For computational convinence (preventing underflow while dealing with small numbers) we work with the $\\log$ of probabilities:\n",
        "\n",
        "\\begin{align} \n",
        "\\log(P(+|\\textbf{x}^{(i)})) &\\propto \\log P(+) + \\sum_{j=1}^p \\log P(x_j^{(i)}|+) \n",
        "\\\\\n",
        "\\log(P(-|\\textbf{x}^{(i)})) &\\propto \\log P(-) + \\sum_{j=1}^p \\log P(x_j^{(i)}|-) \n",
        "\\end{align} \n",
        "\n",
        "Finally we can compute the confidence of our prediction as the log of the ratio of posteriors: \n",
        "$\\log(\\frac{P(\\text{predicted label}|\\textbf{x}^{(i)})}{P(\\text{the other label}|\\textbf{x}^{(i)})})$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1AvG-LXTmPJ",
        "colab_type": "text"
      },
      "source": [
        "**1.8: Implementing NB** Now write a function that takes a row of `xTest` and output a label for it based on NB classification rule. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xu3YKPlzeFLb",
        "colab_type": "code",
        "outputId": "271c7300-d664-4718-e41a-61914c214809",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# classifyNB: \n",
        "#   words - vector of words of the tweet (binary vector)\n",
        "#   logProbWordPresentGivenPositive - log P(x_j = 1|+)\n",
        "#   logProbWordAbsentGivenPositive  - log P(x_j = 0|+)\n",
        "#   logProbWordPresentGivenNegative - log P(x_j = 1|-)\n",
        "#   logProbWordAbsentGivenNegative  - log P(x_j = 0|-)\n",
        "#   logPriorPositive - log P(+)\n",
        "#   logPriorNegative - log P(-)\n",
        "#   returns (label of x according to the NB classification rule, confidence about the label)\n",
        "\n",
        "# Note: you can also change the function definition if you wish to encapsulate all six log probs\n",
        "# as one model; just make sure to follow through below\n",
        "\n",
        "def classifyNB(words, logProbWordPresentGivenPositive, logProbWordAbsentGivenPositive, \n",
        "               logProbWordPresentGivenNegative, logProbWordAbsentGivenNegative, \n",
        "               logPriorPositive, logPriorNegative):\n",
        "  \n",
        "  logProbPositiveGivenTweet = log_prob_sign_given_tweet(words, logPriorPositive, logProbWordPresentGivenPositive, logProbWordAbsentGivenPositive)\n",
        "  logProbNegativeGivenTweet = log_prob_sign_given_tweet(words, logPriorNegative, logProbWordPresentGivenNegative, logProbWordAbsentGivenNegative)\n",
        "  probs = [np.exp(logProbNegativeGivenTweet), np.exp(logProbPositiveGivenTweet)]\n",
        "  confidence = np.log(max(probs)/min(probs))\n",
        "  label = -1 if probs.index(max(probs)) == 0 else 1\n",
        "  return (label, confidence)\n",
        "\n",
        "def log_prob_sign_given_tweet(words, logPriorSign, logProbWordPresentGivenSign, logProbWordAbsentGivenSign):\n",
        "  \"\"\"\n",
        "  A helper method which computes the log probability expression from 1.7.\n",
        "  \"\"\"\n",
        "  words_copy = words.copy()\n",
        "  for i, word in enumerate(words_copy):\n",
        "    if word == 0: # absent\n",
        "      words_copy[i] = logProbWordAbsentGivenSign[i]\n",
        "    else: # present\n",
        "      words_copy[i] = logProbWordPresentGivenSign[i]\n",
        "  return sum(words_copy) + logPriorSign\n",
        "\n",
        "def reverse_word_lookup(index):\n",
        "  \"\"\"\n",
        "  A helper method which gets the word from the word dict associate with the index\n",
        "  \"\"\"\n",
        "  return next(key for key, value in wordDict.items() if value == index)\n",
        "  \n",
        "# Grabs a random tweet and classifies it\n",
        "print(classifyNB(xTest[700, ], logProbWordPresentGivenPositive,logProbWordAbsentGivenPositive,\n",
        "                               logProbWordPresentGivenNegative, logProbWordAbsentGivenNegative,\n",
        "                               logPriorPositive, logPriorNegative))"
      ],
      "execution_count": 219,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 4.37706070095421)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ev7F8osLy3ia",
        "colab_type": "text"
      },
      "source": [
        "**1.9:** Compute the output of `classifyNB` for all test data and output the average error.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_J3BdyCfCVL",
        "colab_type": "code",
        "outputId": "d58db94c-2ab6-4fa3-bd91-0889778ea091",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# testNB: Classify all xTest\n",
        "#   xTest - test data features\n",
        "#   yTest - true label of test data\n",
        "#   logProbWordPresentGivenPositive - log P(x_j = 1|+)\n",
        "#   logProbWordAbsentGivenPositive  - log P(x_j = 0|+)\n",
        "#   logProbWordPresentGivenNegative - log P(x_j = 1|-)\n",
        "#   logProbWordAbsentGivenNegative  - log P(x_j = 0|-)\n",
        "#   logPriorPositive - log P(+)\n",
        "#   logPriorNegative - log P(-)\n",
        "#   returns Average test error\n",
        "def testNB(xTest, yTest, \n",
        "           logProbWordPresentGivenPositive, logProbWordAbsentGivenPositive, \n",
        "           logProbWordPresentGivenNegative, logProbWordAbsentGivenNegative, \n",
        "           logPriorPositive, logPriorNegative):\n",
        "  \n",
        "  # Compute the number of correct matches\n",
        "  matches = 0\n",
        "  for i, tweet in enumerate(xTest):\n",
        "    prediction = classifyNB(tweet, logProbWordPresentGivenPositive,logProbWordAbsentGivenPositive,\n",
        "                               logProbWordPresentGivenNegative, logProbWordAbsentGivenNegative,\n",
        "                               logPriorPositive, logPriorNegative)\n",
        "    actual = yTest[i]\n",
        "    if prediction[0] == actual:\n",
        "      matches += 1\n",
        "\n",
        "  # compute avgErr\n",
        "  avgErr = 1 - matches/len(xTest)\n",
        "  \n",
        "  print(\"Average error of NB is\", avgErr)\n",
        "  return avgErr\n",
        "\n",
        "testNB(xTest, yTest, \n",
        "       logProbWordPresentGivenPositive, logProbWordAbsentGivenPositive, \n",
        "       logProbWordPresentGivenNegative, logProbWordAbsentGivenNegative, \n",
        "       logPriorPositive, logPriorNegative)"
      ],
      "execution_count": 220,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average error of NB is 0.1702702702702703\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.1702702702702703"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 220
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHJ97nqVz8CW",
        "colab_type": "text"
      },
      "source": [
        "**1.10:** Now write an outer wrapper that perform 10 train/test split and compute the mean and standard deviation of the average error across 10 runs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gNGxLT9qzOXl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "outputId": "0e25d71c-4fc0-4c33-b038-f04189f2f41b"
      },
      "source": [
        "from statistics import mean\n",
        "from statistics import stdev\n",
        "\n",
        "# 10 train/test splits\n",
        "def experiment():\n",
        "  error_list = list()\n",
        "  for i in range(10):\n",
        "    # split test data\n",
        "    xTrain, xTest, yTrain, yTest = train_test_split(X, y, test_size = 0.2)\n",
        "\n",
        "    # recompute probabilities\n",
        "    probWordGivenPositive, probWordGivenNegative, priorPositive, priorNegative = compute_distros(xTrain,yTrain)\n",
        "\n",
        "    # recompute log distributions\n",
        "    min_prob = 1 / yTrain.shape[0] #Assume very rare words only appeared once\n",
        "    logProbWordPresentGivenPositive, logProbWordAbsentGivenPositive = compute_logdistros(probWordGivenPositive, min_prob)\n",
        "    logProbWordPresentGivenNegative, logProbWordAbsentGivenNegative = compute_logdistros(probWordGivenNegative, min_prob)\n",
        "    logPriorPositive, logPriorNegative = compute_logdistros(priorPositive, min_prob)\n",
        "\n",
        "    # recompute average error\n",
        "    average_error = testNB(xTest, yTest, \n",
        "        logProbWordPresentGivenPositive, logProbWordAbsentGivenPositive, \n",
        "        logProbWordPresentGivenNegative, logProbWordAbsentGivenNegative, \n",
        "        logPriorPositive, logPriorNegative)\n",
        "    error_list.append(average_error)\n",
        "\n",
        "  print(f'Mean: {mean(error_list)}')\n",
        "  print(f'Standard Deviation: {stdev(error_list)}')\n",
        "\n",
        "experiment()"
      ],
      "execution_count": 221,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average error of NB is 0.18243243243243246\n",
            "Average error of NB is 0.17837837837837833\n",
            "Average error of NB is 0.16486486486486485\n",
            "Average error of NB is 0.14864864864864868\n",
            "Average error of NB is 0.17297297297297298\n",
            "Average error of NB is 0.17567567567567566\n",
            "Average error of NB is 0.16081081081081083\n",
            "Average error of NB is 0.18513513513513513\n",
            "Average error of NB is 0.19189189189189193\n",
            "Average error of NB is 0.14324324324324322\n",
            "Mean: 0.1704054054054054\n",
            "Standard Deviation: 0.015829344133705316\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4RzwSMyd_Hj-",
        "colab_type": "text"
      },
      "source": [
        "**1.11** Finally, let's get to the lab! Now, we need to repeat the experiment above by removing absent words. To do this, I'm just going to take the log probability vectors for absent words and set them all to zero. That way, they don't contribute anything to the sum. In other words, we'd be treating them as probabilities of 1â€”meaning they'd have no effect in a product."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHsExzaU_R8i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "outputId": "791d1eb9-91a6-4757-f944-6d8c832ef367"
      },
      "source": [
        "# 10 train/test splits\n",
        "def experiment_minus_absent_words():\n",
        "  error_list = list()\n",
        "  for i in range(10):\n",
        "    # split test data\n",
        "    xTrain, xTest, yTrain, yTest = train_test_split(X, y, test_size = 0.2)\n",
        "\n",
        "    # recompute probabilities\n",
        "    probWordGivenPositive, probWordGivenNegative, priorPositive, priorNegative = compute_distros(xTrain,yTrain)\n",
        "\n",
        "    # recompute log distributions\n",
        "    min_prob = 1 / yTrain.shape[0] #Assume very rare words only appeared once\n",
        "    logProbWordPresentGivenPositive, logProbWordAbsentGivenPositive = compute_logdistros(probWordGivenPositive, min_prob)\n",
        "    logProbWordPresentGivenNegative, logProbWordAbsentGivenNegative = compute_logdistros(probWordGivenNegative, min_prob)\n",
        "    logPriorPositive, logPriorNegative = compute_logdistros(priorPositive, min_prob)\n",
        "\n",
        "    # ignore absent words by setting their log probabilities to zero\n",
        "    logProbWordAbsentGivenPositive[:] = 0\n",
        "    logProbWordAbsentGivenNegative[:] = 0\n",
        "\n",
        "    # recompute average error\n",
        "    average_error = testNB(xTest, yTest, \n",
        "        logProbWordPresentGivenPositive, logProbWordAbsentGivenPositive, \n",
        "        logProbWordPresentGivenNegative, logProbWordAbsentGivenNegative, \n",
        "        logPriorPositive, logPriorNegative)\n",
        "    error_list.append(average_error)\n",
        "\n",
        "  print(f'Mean: {mean(error_list)}')\n",
        "  print(f'Standard Deviation: {stdev(error_list)}')\n",
        "\n",
        "experiment_minus_absent_words()"
      ],
      "execution_count": 222,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average error of NB is 0.1527027027027027\n",
            "Average error of NB is 0.1945945945945946\n",
            "Average error of NB is 0.1594594594594595\n",
            "Average error of NB is 0.1527027027027027\n",
            "Average error of NB is 0.16621621621621618\n",
            "Average error of NB is 0.15405405405405403\n",
            "Average error of NB is 0.15810810810810816\n",
            "Average error of NB is 0.17432432432432432\n",
            "Average error of NB is 0.16621621621621618\n",
            "Average error of NB is 0.20540540540540542\n",
            "Mean: 0.16837837837837838\n",
            "Standard Deviation: 0.01821068165180867\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PRT8O9yQH-O",
        "colab_type": "text"
      },
      "source": [
        "To be honest, I don't see much of a difference. Error seems about the same. On average, we're missing about 17% of predictions. Whether or no we include the probabilities of absent words doesn't seem to matter. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkUo3uXbHVqi",
        "colab_type": "text"
      },
      "source": [
        "**2.0** Stop Words: At this point, we get to experiment with word removal. In this case, we're going to try removing the top 25, 50, 100, and 200 words. To do that, we'll start by identifying the indices of the most frequent words:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5JT03LwH0AH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def top_words(tweets, count):\n",
        "  \"\"\"\n",
        "  A helper method which helps us determine the indices of the top count # of words.\n",
        "\n",
        "  :param tweets: a list of tweets in the form of X (e.g. xTrain, xTest, etc.)\n",
        "  :param counts: the number of word indices to extract\n",
        "  \"\"\"\n",
        "  word_counts = np.sum(tweets, axis=0)\n",
        "  top_word_indices = np.argpartition(word_counts, -count)[-count:]\n",
        "  return top_word_indices\n",
        "\n",
        "def remove_stop_words(indices, *distros):\n",
        "  for index in indices:\n",
        "    for distro in distros:\n",
        "      distro[index] = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LpjHoyz5LiUe",
        "colab_type": "text"
      },
      "source": [
        "**2.1** At this point, we'll rewrite our experiment code to remove the top count of stop words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ap0cIR3VLzPC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 10 train/test splits\n",
        "def experiment_minus_stop_words(count):\n",
        "  error_list = list()\n",
        "  for i in range(10):\n",
        "    # split test data\n",
        "    xTrain, xTest, yTrain, yTest = train_test_split(X, y, test_size = 0.2)\n",
        "\n",
        "    # recompute probabilities\n",
        "    probWordGivenPositive, probWordGivenNegative, priorPositive, priorNegative = compute_distros(xTrain,yTrain)\n",
        "\n",
        "    # recompute log distributions\n",
        "    min_prob = 1 / yTrain.shape[0] #Assume very rare words only appeared once\n",
        "    logProbWordPresentGivenPositive, logProbWordAbsentGivenPositive = compute_logdistros(probWordGivenPositive, min_prob)\n",
        "    logProbWordPresentGivenNegative, logProbWordAbsentGivenNegative = compute_logdistros(probWordGivenNegative, min_prob)\n",
        "    logPriorPositive, logPriorNegative = compute_logdistros(priorPositive, min_prob)\n",
        "\n",
        "    # Computes the top words and removes them from the distribution\n",
        "    top_word_indices = top_words(xTrain, count)\n",
        "    remove_stop_words(top_word_indices, logProbWordPresentGivenPositive, \n",
        "                      logProbWordAbsentGivenPositive, logProbWordPresentGivenNegative, \n",
        "                      logProbWordAbsentGivenNegative)\n",
        "\n",
        "    # recompute average error\n",
        "    average_error = testNB(xTest, yTest, \n",
        "        logProbWordPresentGivenPositive, logProbWordAbsentGivenPositive, \n",
        "        logProbWordPresentGivenNegative, logProbWordAbsentGivenNegative, \n",
        "        logPriorPositive, logPriorNegative)\n",
        "    error_list.append(average_error)\n",
        "\n",
        "  print(f'Mean: {mean(error_list)}')\n",
        "  print(f'Standard Deviation: {stdev(error_list)}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6B0GHAp2NaEg",
        "colab_type": "text"
      },
      "source": [
        "**2.2** Cool! Now, we can just run this experiment 4 times. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Plqd4eg4NTme",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "outputId": "56902f1f-202b-463d-8113-8b1a55719d28"
      },
      "source": [
        "experiment_minus_stop_words(25)"
      ],
      "execution_count": 225,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average error of NB is 0.18243243243243246\n",
            "Average error of NB is 0.16756756756756752\n",
            "Average error of NB is 0.16486486486486485\n",
            "Average error of NB is 0.17432432432432432\n",
            "Average error of NB is 0.20405405405405408\n",
            "Average error of NB is 0.16756756756756752\n",
            "Average error of NB is 0.16486486486486485\n",
            "Average error of NB is 0.16216216216216217\n",
            "Average error of NB is 0.18918918918918914\n",
            "Average error of NB is 0.19864864864864862\n",
            "Mean: 0.17756756756756756\n",
            "Standard Deviation: 0.015184924539971747\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOtj_1kKNULL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "outputId": "47e9d094-4df1-4d5b-8bba-44e05ed7c285"
      },
      "source": [
        "experiment_minus_stop_words(50)"
      ],
      "execution_count": 226,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average error of NB is 0.20270270270270274\n",
            "Average error of NB is 0.23108108108108105\n",
            "Average error of NB is 0.22837837837837838\n",
            "Average error of NB is 0.21756756756756757\n",
            "Average error of NB is 0.23108108108108105\n",
            "Average error of NB is 0.2391891891891892\n",
            "Average error of NB is 0.2189189189189189\n",
            "Average error of NB is 0.2256756756756757\n",
            "Average error of NB is 0.22297297297297303\n",
            "Average error of NB is 0.20405405405405408\n",
            "Mean: 0.22216216216216217\n",
            "Standard Deviation: 0.011732483312504278\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_dEgYjFDNVM0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "outputId": "270c9053-6be9-49d6-d8e0-bd295289e2f1"
      },
      "source": [
        "experiment_minus_stop_words(100)"
      ],
      "execution_count": 227,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average error of NB is 0.2648648648648648\n",
            "Average error of NB is 0.2364864864864865\n",
            "Average error of NB is 0.2959459459459459\n",
            "Average error of NB is 0.25\n",
            "Average error of NB is 0.27567567567567564\n",
            "Average error of NB is 0.2945945945945946\n",
            "Average error of NB is 0.26216216216216215\n",
            "Average error of NB is 0.25\n",
            "Average error of NB is 0.25135135135135134\n",
            "Average error of NB is 0.25810810810810814\n",
            "Mean: 0.2639189189189189\n",
            "Standard Deviation: 0.019552351690238706\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zaJOnGEhNWJr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "outputId": "f4291c5c-5d7e-4d00-b18b-bac79555c573"
      },
      "source": [
        "experiment_minus_stop_words(200)"
      ],
      "execution_count": 228,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average error of NB is 0.3202702702702702\n",
            "Average error of NB is 0.2945945945945946\n",
            "Average error of NB is 0.2716216216216216\n",
            "Average error of NB is 0.33108108108108103\n",
            "Average error of NB is 0.2716216216216216\n",
            "Average error of NB is 0.31351351351351353\n",
            "Average error of NB is 0.3027027027027027\n",
            "Average error of NB is 0.30810810810810807\n",
            "Average error of NB is 0.2959459459459459\n",
            "Average error of NB is 0.32837837837837835\n",
            "Mean: 0.3037837837837838\n",
            "Standard Deviation: 0.020952483722501938\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EicL_ITTQwpn",
        "colab_type": "text"
      },
      "source": [
        "When we removed the first 25 stop words, we didn't see much of a change in performance. However, as we increased the number of words we removed, we started to lose valuable semantic information. By the time we removed 200 words, we saw a significant drop in performance. "
      ]
    }
  ]
}