{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lab2.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMh5H+o/nyR83PJjwGhufww",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jrg94/CSE5522/blob/lab2/lab2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3Yg-tYejPDe",
        "colab_type": "text"
      },
      "source": [
        "# CSE 5522 - Lab 2\n",
        "By Jeremy Grifski"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5c9Rh8njZyn",
        "colab_type": "text"
      },
      "source": [
        "In this lab, we'll be taking a look at sentiment analysis of tweet data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OqVwOqZwjn79",
        "colab_type": "text"
      },
      "source": [
        "## Part 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQ4zl1LukMCk",
        "colab_type": "text"
      },
      "source": [
        "**1.0** Set up the environment (you can click on the play button below to import the appropriate modules)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q7jdntWtjOJn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jagDaH_IkH3m",
        "colab_type": "text"
      },
      "source": [
        "**1.1** Read the data from GitHub into a pandas dataframe.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wg6ioz-BkW4e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TweetUrl='https://github.com/aasiaeet/cse5522data/raw/master/db3_final_clean.csv'\n",
        "tweet_dataframe=pd.read_csv(TweetUrl)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ukzdqt07lx6f",
        "colab_type": "text"
      },
      "source": [
        "**1.2** Print out the top of the dataframe to make sure that the data loaded correctly. It should be a data table with three columns (weight, tweet, label), and 3697 rows."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LEWVXZaYl0k1",
        "colab_type": "code",
        "outputId": "e1ee9d67-182a-4947-9143-b43730bfe53c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        }
      },
      "source": [
        "display(tweet_dataframe.shape)\n",
        "tweet_dataframe.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(3697, 3)"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>weight</th>\n",
              "      <th>tweet</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.0000</td>\n",
              "      <td>it is very cold out want it to be warmer</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.7698</td>\n",
              "      <td>dammmmmmm its pretty cold this morning   burr lol</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.6146</td>\n",
              "      <td>why does halsey have to be so far away think m...</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.9356</td>\n",
              "      <td>dammit stop being so cold so can work out</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.0000</td>\n",
              "      <td>its too freakin cold</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   weight                                              tweet  label\n",
              "0  1.0000          it is very cold out want it to be warmer      -1\n",
              "1  0.7698  dammmmmmm its pretty cold this morning   burr lol     -1\n",
              "2  0.6146  why does halsey have to be so far away think m...     -1\n",
              "3  0.9356         dammit stop being so cold so can work out      -1\n",
              "4  1.0000                               its too freakin cold     -1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNd7KaYnqBFT",
        "colab_type": "text"
      },
      "source": [
        "Labels are -1 and +1 for negative and positive sentiments respectively. Multiple judges have been asked to choose a label for a tweet (this is an example of crowd-sourcing) from five possible labels:\n",
        "\n",
        "- Tweet is not relevant to weather.\n",
        "- I can't tell the sentiment.\n",
        "- Neutral: author just sharing information.\n",
        "- Positive\n",
        "- Negative\n",
        "\n",
        "\n",
        "The majority vote was picked as the label and its ratio was set as the weight of the tweet. So for the tweet in row 2 above, 61% of judges voted that the label is negative.\n",
        "\n",
        "Note that tweets have been pre-processed (or cleaned). For example, :) and :( :) were replaced with \"sad\" and \"smiley\" and numbers with \"num\", etc. You can go further (as we ask in 1.12) and remove the stop words, i.e., repetitive non-informative words such as am, is, and are."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dm5TRWzaqG-b",
        "colab_type": "text"
      },
      "source": [
        "**1.3** In the next step, we should build our feature matrix by converting the string of words to a vector of numeric values.\n",
        "\n",
        "First we need to assign a unique id to each word and create the feature matrix with correct size:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVVzeHL-qMOb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# wordDict maps words to id\n",
        "# X is the document-word matrix holding the presence/absence of words in each tweet\n",
        "wordDict = {}\n",
        "idCounter = 0\n",
        "for i in range(tweet_dataframe.shape[0]):\n",
        "  allWords = tweet_dataframe.iloc[i,1].split(\" \")\n",
        "  for word in allWords:\n",
        "    if word not in wordDict:\n",
        "      wordDict[word] = idCounter\n",
        "      idCounter += 1\n",
        "X = np.zeros((tweet_dataframe.shape[0], idCounter),dtype='float')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_e8F8XJpqzrN",
        "colab_type": "text"
      },
      "source": [
        "Checking head of the dictionary:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwPwu3lDq1WL",
        "colab_type": "code",
        "outputId": "5ea7da78-d4de-4cd4-9d13-ef3d98e9eccf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "source": [
        "dict(list(wordDict.items())[0:10])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'': 9,\n",
              " 'be': 7,\n",
              " 'cold': 3,\n",
              " 'is': 1,\n",
              " 'it': 0,\n",
              " 'out': 4,\n",
              " 'to': 6,\n",
              " 'very': 2,\n",
              " 'want': 5,\n",
              " 'warmer': 8}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3XUpzZ_q3WM",
        "colab_type": "text"
      },
      "source": [
        "**1.4** The simplest way of coding a tweet to numbers is to mark the occurrence of a word, and forget about its frequency in the document (tweet). This works well with tweets as there are not many repetitive words in a single tweet. So let's fill the document-word matrix:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yuGZb8uUq6Mr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(tweet_dataframe.shape[0]):\n",
        "  allWords = tweet_dataframe.iloc[i,1].split(\" \")\n",
        "  for word in allWords:\n",
        "    X[i, wordDict[word]]  = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FmzAkh2ErEkk",
        "colab_type": "text"
      },
      "source": [
        "Now we check if the number of words are correct:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d08d8GrHrGcr",
        "colab_type": "code",
        "outputId": "b9ace6f3-be67-4960-cc0a-be69c086f56f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "np.sum(X[0:5, ], axis = 1)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([10.,  9., 17.,  9.,  4.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wWc1zWxrH-1",
        "colab_type": "text"
      },
      "source": [
        "Finally, we extract the labels from the dataframe:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNBuFCJzrJ8E",
        "colab_type": "code",
        "outputId": "03afa994-4159-496c-bb31-d6a1aa85d4e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y = np.array(tweet_dataframe.iloc[:,2])\n",
        "y[0:5]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-1, -1, -1, -1, -1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xLNy-zArMAt",
        "colab_type": "text"
      },
      "source": [
        "Let's compute the total number of positive and negative tweets:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tawyq0NJrOLc",
        "colab_type": "code",
        "outputId": "e4e10ec2-07e8-4a00-8bcb-2ec7624944ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "numNeg = np.sum(y<0)\n",
        "numPos = np.sum(y>=0) #len(y) - numNeg\n",
        "probNeg = numNeg / (numNeg + numPos)\n",
        "probPos = 1 - probNeg\n",
        "display(numNeg, numPos, probNeg, probPos)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "1650"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "2047"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0.4463078171490398"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0.5536921828509602"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WY-ukqndrP9s",
        "colab_type": "text"
      },
      "source": [
        "So samples 0:1649 are negative and 1650:-1 are positive."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQHG-cg8Rm3I",
        "colab_type": "text"
      },
      "source": [
        "**1.5** Train/Test Split Now with do the 20/80 split and learn the word probabilities using the 80 % part and test the NB performance on the 20 % part."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sb-bUT-URsRd",
        "colab_type": "code",
        "outputId": "d126038e-7beb-40d3-dc26-42496d794257",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "xTrain, xTest, yTrain, yTest = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
        "display(xTrain.shape, xTrain, xTest.shape, xTest, yTrain.shape, yTest.shape)\n",
        "#Note: random_state=0 fixes the random seed so we get the same split every run. Don't use this below"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(2957, 5989)"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(740, 5989)"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([[0., 1., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [1., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(2957,)"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(740,)"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nt2heJYDRvzn",
        "colab_type": "text"
      },
      "source": [
        "**1.6** Computing Probabilities by Counting Now the real work begins. Write the code that, from the train feature matrix xTrain computes the needed word probabilites, i.e.,  P(word|label)  where label is + or - and word is any of the words saved in the wordDict:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWwubhL9R1I_",
        "colab_type": "code",
        "outputId": "08cde680-5f76-4df3-8e8e-9734473eb88d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "# compute three distributions (four variables):\n",
        "def compute_distros(x,y):\n",
        "  # probWordGivenPositive: P(word|Sentiment = +ive)\n",
        "  probWordGivenPositive=np.sum(x[y>=0,:],axis=0) #Sum each word (column) to count how many times each word shows up (in positive examples)\n",
        "  probWordGivenPositive=probWordGivenPositive/np.sum(y>=0) #Divide by total number of (positive) examples to give distribution\n",
        "\n",
        "  # probWordGivenNegative: P(word|Sentiment = -ive)\n",
        "  probWordGivenNegative=np.sum(x[y<0,:],axis=0)\n",
        "  probWordGivenNegative=probWordGivenNegative/np.sum(y<0)\n",
        "\n",
        "  # priorPositive: P(Sentiment = +ive)\n",
        "  priorPositive = np.sum(y>=0)/y.shape[0] #Number of positive examples vs. all examples\n",
        "  # priorNegative: P(Sentiment = -ive)\n",
        "  priorNegative = 1 - priorPositive\n",
        "  #  (note these last two form one distribution)\n",
        "\n",
        "  return probWordGivenPositive, probWordGivenNegative, priorPositive, priorNegative\n",
        "\n",
        "# compute distributions here\n",
        "probWordGivenPositive, probWordGivenNegative, priorPositive, priorNegative = compute_distros(xTrain,yTrain)\n",
        "\n",
        "# checking the results\n",
        "display(probWordGivenPositive[0:5])\n",
        "display(probWordGivenNegative[0:5])\n",
        "display(priorPositive, priorNegative)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([0.1185006 , 0.20737606, 0.01088271, 0.01451028, 0.10217654])"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([0.14504988, 0.19493477, 0.00537222, 0.09669992, 0.13967767])"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0.5593506932702063"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0.44064930672979374"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JUFU8eUQ_8gC",
        "colab_type": "text"
      },
      "source": [
        "Note that you only needed to compute $P(word = 1| +)$ or $P(word = 1| -)$ and the probabilities of the word being absent from a tweet is just 1 minus those probabilities. \n",
        "\n",
        "However, as we see in 1.7, for convenience, we will also want to compute $log P(word = 1 | +)$, $log P(word = 0 | +)$, $log P(word = 1 | -)$ and $log P(word = 0 | -)$.  Also we should compute the log priors.  Let's do so now.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1HLcaaDTiwF0",
        "colab_type": "code",
        "outputId": "af7fa4dd-6db7-48d9-9070-4efe400fa3c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "# compute the following:\n",
        "# logProbWordPresentGivenPositive\n",
        "# logProbWordAbsentGivenPositive\n",
        "# logProbWordPresentGivenNegative\n",
        "# logProbWordAbsentGivenNegative\n",
        "# logPriorPositive\n",
        "# logPriorNegative\n",
        "def compute_logdistros(distros, min_prob):\n",
        "  if True:\n",
        "    #Assume missing words are simply very rare\n",
        "    #So, assign minimum probability to very small elements (e.g. 0 elements)\n",
        "    distros=np.where(distros>=min_prob,distros,min_prob)\n",
        "    #Also need to consider minimum probability for \"not\" distribution\n",
        "    distros=np.where(distros<=(1-min_prob),distros,1-min_prob)\n",
        "\n",
        "    return np.log(distros), np.log(1-distros)\n",
        "  else:\n",
        "    #Ignore missing words (assume they have P==1, i.e. force log 0 to 0)\n",
        "    return np.log(np.where(distros>0,distros,1)), np.log(np.where(distros<1,1-distros,1))\n",
        "\n",
        "min_prob = 1/yTrain.shape[0] #Assume very rare words only appeared once\n",
        "logProbWordPresentGivenPositive, logProbWordAbsentGivenPositive = compute_logdistros(probWordGivenPositive,min_prob)\n",
        "logProbWordPresentGivenNegative, logProbWordAbsentGivenNegative = compute_logdistros(probWordGivenNegative,min_prob)\n",
        "logPriorPositive, logPriorNegative = compute_logdistros(priorPositive,min_prob)\n",
        "\n",
        "# Did this work, or did you get an error?  (Read below.)\n",
        "display(logProbWordPresentGivenPositive[0:5])\n",
        "display(logProbWordAbsentGivenPositive[0:5])\n",
        "display(logProbWordPresentGivenNegative[0:5])\n",
        "display(logProbWordAbsentGivenNegative[0:5])\n",
        "display(logPriorPositive, logPriorNegative)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([-2.13283722, -1.57322143, -4.52058012, -4.23289805, -2.28105316])"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([-0.12613096, -0.23240639, -0.01094236, -0.01461658, -0.10778182])"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([-1.93067756, -1.63509031, -5.22651443, -2.33614267, -1.96841789])"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([-0.15671216, -0.21683197, -0.0053867 , -0.10170047, -0.15044815])"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "-0.5809786442688406"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "-0.819505942727632"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXVQ7ZHAkH1u",
        "colab_type": "text"
      },
      "source": [
        "You likely received an error when you tried to take $log(0)$ at some point.  Can your group think of a way to avoid taking $log(0)$?  Check in with your instructor/TA to see if what you're thinking will work.  Implement that change in your code above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxdbYsu9E8av",
        "colab_type": "text"
      },
      "source": [
        "**1.7: Math of NB** Here we provide the derivation of NB when we want to classify the $i$th tweet $\\textbf{x}^{(i)}$ and the size of dictionary is $p$, i.e., each tweet is a binary vector of size $p$ as $\\textbf{x}^{(i)} = (x_1^{(i)},\\dots, x_p^{(i)})$. \n",
        "\n",
        "Note that we computed $P(x_j^{(i)} = 1|+)$ and $P(x_j^{(i)} = 1|-)$ in above code from `xTrain` and now want to classify `xTest` samples.\n",
        "\n",
        "**Classification Rule:** For each tweet $i$ NB classifier assigns label + if $P(+|\\textbf{x}^{(i)}) > P(-|\\textbf{x}^{(i)})$ and negative otherwise. \n",
        "\n",
        "These posterior probabilities can be computed using prior probabilities (that we got from `xTrain`) and Bayes rule as follows: \n",
        "\n",
        "\\begin{align}\n",
        "P(+|\\textbf{x}^{(i)}) &= \\alpha P(\\{\\textbf{x}^{(i)}\\}_{i=1}^n | +)P(+) \n",
        "\\\\\n",
        "(\\text{NB Assumption}) &= \\alpha P(+) \\prod_{j=1}^p P(x_j^{(i)}|+)\n",
        "\\end{align}\n",
        "\n",
        "For computational convinence (preventing underflow while dealing with small numbers) we work with the $\\log$ of probabilities:\n",
        "\n",
        "\\begin{align} \n",
        "\\log(P(+|\\textbf{x}^{(i)})) &\\propto \\log P(+) + \\sum_{j=1}^p \\log P(x_j^{(i)}|+) \n",
        "\\\\\n",
        "\\log(P(-|\\textbf{x}^{(i)})) &\\propto \\log P(-) + \\sum_{j=1}^p \\log P(x_j^{(i)}|-) \n",
        "\\end{align} \n",
        "\n",
        "Finally we can compute the confidence of our prediction as the log of the ratio of posteriors: \n",
        "$\\log(\\frac{P(\\text{predicted label}|\\textbf{x}^{(i)})}{P(\\text{the other label}|\\textbf{x}^{(i)})})$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1AvG-LXTmPJ",
        "colab_type": "text"
      },
      "source": [
        "**1.8: Implementing NB** Now write a function that takes a row of `xTest` and output a label for it based on NB classification rule. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xu3YKPlzeFLb",
        "colab_type": "code",
        "outputId": "a75de4e3-1e3d-44aa-8efa-e54912b2bd58",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# classifyNB: \n",
        "#   words - vector of words of the tweet (binary vector)\n",
        "#   logProbWordPresentGivenPositive - log P(x_j = 1|+)\n",
        "#   logProbWordAbsentGivenPositive  - log P(x_j = 0|+)\n",
        "#   logProbWordPresentGivenNegative - log P(x_j = 1|-)\n",
        "#   logProbWordAbsentGivenNegative  - log P(x_j = 0|-)\n",
        "#   logPriorPositive - log P(+)\n",
        "#   logPriorNegative - log P(-)\n",
        "#   returns (label of x according to the NB classification rule, confidence about the label)\n",
        "\n",
        "# Note: you can also change the function definition if you wish to encapsulate all six log probs\n",
        "# as one model; just make sure to follow through below\n",
        "\n",
        "def classifyNB(words, logProbWordPresentGivenPositive, logProbWordAbsentGivenPositive, \n",
        "               logProbWordPresentGivenNegative, logProbWordAbsentGivenNegative, \n",
        "               logPriorPositive, logPriorNegative):\n",
        "  \n",
        "  logProbPositiveGivenTweet = log_prob_sign_given_tweet(words, logPriorPositive, logProbWordPresentGivenPositive, logProbWordAbsentGivenPositive)\n",
        "  logProbNegativeGivenTweet = log_prob_sign_given_tweet(words, logPriorNegative, logProbWordPresentGivenNegative, logProbWordAbsentGivenNegative)\n",
        "  probs = [np.exp(logProbNegativeGivenTweet), np.exp(logProbPositiveGivenTweet)]\n",
        "  confidence = np.log(max(probs)/min(probs))\n",
        "  label = -1 if probs.index(max(probs)) == 0 else 1\n",
        "  return (label, confidence)\n",
        "\n",
        "def log_prob_sign_given_tweet(words, logPriorSign, logProbWordPresentGivenSign, logProbWordAbsentGivenSign):\n",
        "  words_copy = words.copy()\n",
        "  for i, word in enumerate(words_copy):\n",
        "    if word == 0: # absent\n",
        "      words_copy[i] = logProbWordAbsentGivenSign[i]\n",
        "    else: # present\n",
        "      # print(next(key for key, value in wordDict.items() if value == i)) # Prints present words\n",
        "      words_copy[i] = logProbWordPresentGivenSign[i]\n",
        "  return sum(words_copy) + logPriorSign\n",
        "  \n",
        "# Grabs a random tweet and classifies it\n",
        "print(classifyNB(xTest[700, ], logProbWordPresentGivenPositive,logProbWordAbsentGivenPositive,\n",
        "                               logProbWordPresentGivenNegative, logProbWordAbsentGivenNegative,\n",
        "                               logPriorPositive, logPriorNegative))"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 4.37706070095421)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ev7F8osLy3ia",
        "colab_type": "text"
      },
      "source": [
        "**1.9:** Compute the output of `classifyNB` for all test data and output the average error.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_J3BdyCfCVL",
        "colab_type": "code",
        "outputId": "bcd53b38-7ed0-4931-d938-45d8c86fbcb4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# testNB: Classify all xTest\n",
        "#   xTest - test data features\n",
        "#   yTest - true label of test data\n",
        "#   logProbWordPresentGivenPositive - log P(x_j = 1|+)\n",
        "#   logProbWordAbsentGivenPositive  - log P(x_j = 0|+)\n",
        "#   logProbWordPresentGivenNegative - log P(x_j = 1|-)\n",
        "#   logProbWordAbsentGivenNegative  - log P(x_j = 0|-)\n",
        "#   logPriorPositive - log P(+)\n",
        "#   logPriorNegative - log P(-)\n",
        "#   returns Average test error\n",
        "def testNB(xTest, yTest, \n",
        "           logProbWordPresentGivenPositive, logProbWordAbsentGivenPositive, \n",
        "           logProbWordPresentGivenNegative, logProbWordAbsentGivenNegative, \n",
        "           logPriorPositive, logPriorNegative):\n",
        "\n",
        "  # compute avgErr\n",
        "  avgErr = 0.0\n",
        "  \n",
        "  print(\"Average error of NB is\", avgErr)\n",
        "  return avgErr\n",
        "\n",
        "testNB(xTest, yTest, \n",
        "       logProbWordPresentGivenPositive, logProbWordAbsentGivenPositive, \n",
        "       logProbWordPresentGivenNegative, logProbWordAbsentGivenNegative, \n",
        "       logPriorPositive, logPriorNegative)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average error of NB is 0.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHJ97nqVz8CW",
        "colab_type": "text"
      },
      "source": [
        "**1.10:** Now write an outer wrapper that perform 10 train/test split and compute the mean and standard deviation of the average error across 10 runs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gNGxLT9qzOXl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 10 train/test splits\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}